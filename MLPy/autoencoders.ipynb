{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "\n",
    "By Definition, an autoencoder is a neural network trained to downsize then reconstruct its input data from a latent representation. A convolutional autoencoder uses a CNN encoder to compress the input into the a latent representation and a deconvolutional (transposed convolution) decoder to reconstruct the input from the latent representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization notes: \n",
    "\n",
    "- ensure images are stacked as a tensor instead of parsed in as a list of PIL images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, gzip, math, os, time, shutil, torch, matplotlib.pyplot as plt, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cv_project/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pathlib as Path\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from typing import Mapping \n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/cifar-10-batches-py/data_batch_1', 'rb') as f:\n",
    "    dataset_dict = pickle.load(f, encoding='bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict[b'data'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'training batch 1 of 5'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict[b'batch_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict[b'labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = dataset_dict[b'data']\n",
    "y_train = dataset_dict[b'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(x_train, dtype=torch.float32).view(-1, 3, 32, 32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 3, 32, 32])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4463,  0.1121,  0.2813,  0.3727,  0.2054,  0.0144,  0.2675,  0.2481,\n",
       "         -0.4389,  0.0106]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample cnn\n",
    "from torch import nn \n",
    "\n",
    "simple_cnn = nn.Sequential(\n",
    "    nn.Conv2d(3, 4, kernel_size=3, stride=2, padding=1),    # 14x14x4\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(4, 8, kernel_size=3, stride=2, padding=1),    # 7x7x8\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1),    # 4x4x16\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),    # 2x2x16\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),    # 1x1x10\n",
    "    nn.Flatten(),\n",
    ")\n",
    "\n",
    "simple_cnn(x_train[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x_train.reshape(10000, 3, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 3, 32, 32])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_torch_image(img):\n",
    "    img = img.permute(1, 2, 0)\n",
    "    img = img.numpy().astype(np.uint8)\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAVSklEQVR4nO3cy49k93Uf8HPr1V09PS9yKJMWLYW2LFmRCSkKkIedADaMPAADWQVeGN546b/Le++8MpDAhqWFAYuS4sSkAkp8ecihZoac6Z7urq6qe7MQcrzUOQLLHgKfz/rMmV/denz7Lu53mKZpCgCIiNk/9wEAeH4IBQCSUAAgCQUAklAAIAkFAJJQACAJBQDSojr4wmJ+uFN0n58bhvLobHa43Bsa54g47FkO+Qxi93V25w/l0OfuzI/NS3LIJ0o7n5VDXpPn5XNyaOM4tuY7789+v2/tfnhx9XNn3CkAkIQCAEkoAJCEAgBJKACQhAIASSgAkIQCAEkoAJCEAgBJKACQyt1HB+0peY46UKbGUTqzEb0+m+4lOWRXTvfdOWBLVnROM82aJ29e9NY173YItaafH63fiaH3qR2eo0/51PjyH/K3cz7/7L9t7hQASEIBgCQUAEhCAYAkFABIQgGAJBQASEIBgCQUAEhCAYBUrrk4pINWaBxQ/6H7xr/o1iIMh8v37tszNP7WOOQ7Px6ytqJpaL7Sz+lXoqX/Gg/5DjV1O24O5BC/ne4UAEhCAYAkFABIQgGAJBQASEIBgCQUAEhCAYAkFABIQgGAJBQASOXuo7HbUnPAapDO6m43SGe+/RKHZXl0Nm/mdfMw476zf947yqzeUbOIfWv31Hmhzfd+jLE1PxvqZx+G3u7pgN06s1nrG9Ta3fu+NbuMDtjvNXVrlQ73MWxdlmn87Pug3CkAkIQCAEkoAJCEAgBJKACQhAIASSgAkIQCAEkoAJCEAgCpXHMR3dqFhn5dRKOKovmM+WxoVDo0ait+Nn5cnp0v6rMREfOb91rzL3/zt8qzN77w1dbuf3h8VZ49e/hBa/fswd+XZxef/KS1e7h+0prfNj5a437X2t2p0OhXUdSrEdoVDS0HXd7TuCYREVOroqP5/rQ6NHr1KRXuFABIQgGAJBQASEIBgCQUAEhCAYAkFABIQgGAJBQASEIBgCQUAEj17qOp1w3S7Rxq7T7Y5oiY1buPhsWqtXp+dFSePbn5Umv3137vD1rzt//V75ZnH330sLX7eLkpz17e/Epr9+beN+uzjZ6kiIjj97/Tml9c1Hub9kP9mkREzKb6V3M2Nnt7hm15djxAt84/an6Tey+z2dvU7Sc6nE6v0jD77P+ud6cAQBIKACShAEASCgAkoQBAEgoAJKEAQBIKACShAEASCgCk8rP049h73H12gMevfxHduo1h2pVnZ2OvumAx1Gsxbq175753db81v/r7vyrPXj7pvc6vHt0qz57Nb7R2vz/WKxo+3L3Q2n3+0u+05o+v6zUaq8dvtnYvN2fl2XHo9T/sGrUYQ9S/D///X9R1eyuadR697S2d37fub2fLAfo2no9fbgCeC0IBgCQUAEhCAYAkFABIQgGAJBQASEIBgCQUAEhCAYAkFABI5e6jbsXGMDWaR5r9RJ0ka50jImKq95RM++vW6u3l0/Lsowe9zplHb/b6VX7nW6+XZ1+99eXW7rPtw/Ls/Yf/u7X74icflmfnu+PW7suv/8fW/Kdf+L3y7PWPf9jaffJ//7w8uzp7u7V7tm18J6bed3PqNA4N+9bubvdR6yjN36Cp8bvS3X2oc1S5UwAgCQUAklAAIAkFAJJQACAJBQCSUAAgCQUAklAAIAkFAFK55mLWfJx61nm0u7m789B46xzRf5K+Y7avP9Y/Xp63dr/z3jut+bNX6mdZbL7X2v3s8bY8O15dtXb/xmJdnr31yiut3T+996Q1/919vVrkH5a/2to93Pl2ffbqk9bu+e5+eXYa5q3d01R/7yN61Syd2oquQ9RF/FNQcwHAQQkFAJJQACAJBQCSUAAgCQUAklAAIAkFAJJQACAJBQCSUAAglbuPotkh1DF0+4kOeZYDzf5svv4vls3l09jpnIm4+2K9Q+jrt5at3X/1Rr1b52R9q7V71ijA2V581Np99IM/a83/5vr75dmTeK21+/2oX5eLm99s7V5vz8qz822vD6rzsR2n7jeo15XU6QU65G9Qt5/on7uHyZ0CAEkoAJCEAgBJKACQhAIASSgAkIQCAEkoAJCEAgBJKACQhAIAqdx9NGt2g7TmD9g70u00mWaNTpNZL1OHeX1+Wsxbuy+bTUw/+mhTnv1P/7rXrfPt4U559oOHl63d7z2od/E8unza2n29+7Q1f3d4qzz779Yft3a/dPpSefbHi3ut3bPZ6+XZ6eEbrd3j7nF9uF1h1vu+jWOvK6mj00/U7TLqnPsQPUnuFABIQgGAJBQASEIBgCQUAEhCAYAkFABIQgGAJBQASEIBgFSuuVgsyqMRcdgqiqFRL9Gt54hOdUWjEiMiYj6vV1fMmzUXs8W6Nf/Ge/XH4//P8l+2dv+bP/7j8uyv3H/U2n30vb+rD7/7Tmv37vqiN395Vp4dz3o1F986+qA8++Ub9cqSiIg34nZ59vzqtLV7fl6vIdnue5/xcbpuzX9edX4P1VwAcFBCAYAkFABIQgGAJBQASEIBgCQUAEhCAYAkFABIQgGAJBQASOVCo1//1m+3Fo+NTo7tbmzt3o/1+XHf2z02dnd7R3rTvV6l2XzVmj8f6/v/9M//trU77r5WHv32b77eWv1bL9R3v/bJ49bui7Pe/NnD++XZ84cftnZPT35anl2d3G3tvnn1xfLsX3yntTqu3t+XZ5dXD1u7d1OvK6nzHZqm3u/EITqHfhGzTldbdednvhGAzy2hAEASCgAkoQBAEgoAJKEAQBIKACShAEASCgAkoQBAGqbi89o//Ls3W4vHsVFzse89Mn693dVnr69bu/e7+u79vv5If0SzQqNx/X62uzUeY+Psjx/36h/WJ+vy7L/4lVdau6f9tjy73fYuyjvvvNuaf+/d+vzNk3KjTEREjPvL8uzFxXlrd6xul0d/+F7v+/O33/tf5dnNT99q7Z5vPm7Nj/v62ZttOK1ajHYdzgErND785OnPnXGnAEASCgAkoQBAEgoAJKEAQBIKACShAEASCgAkoQBAEgoAJKEAQCoXsjz+4AetxdNsKM+uVvWunIiIF1+8V56d3+x1zgzDsjy7XN5o7Z7P552TtHaPzb6U3a7efTTuX2jtjqi/zo8/6vXZPPn0SXn2/PyitXu/qfcNRUTcPl2VZ2er3vv5xg9+XJ79wffrfUMREfN9vd9rta5/HyIi1uNxeXY86fVebY7utuanyw/Ls/PLR73dje9ns1apd44D9CS5UwAgCQUAklAAIAkFAJJQACAJBQCSUAAgCQUAklAAIAkFAFK5A+L7f/Pd1uKTWzcb070KgHsvvlQ/x8lJa/d2W68AuHHjtLV7va7XefSuSMQ09R6m71RuLBa9qoOjo/rrvHvaqf6IWM/rNQofXD5u7f7Cq3da86tl/XM4TfVakYiI5VT/HL715tut3Q/uf1SenR73qj9iqNcuLBe9epvZ6sXeWU7qv0G7Xb0+JSJit73unaXjs2+uaHGnAEASCgAkoQBAEgoAJKEAQBIKACShAEASCgAkoQBAEgoAJKEAQCp3H50c32gt3m8bw2Ov7OPq/Ko8u17Uu3IiItar+vzm2UVv92JVnj250etsiql3DYeh3q40m/eamKZd/f3ZbZstT7v66xym3u51o7MpIuKLX/zl8uzZk0et3aer8lczjuqjERExm9d7mIap93fjdlt/7zuzERHD5aet+U6H0DjWu6YiImJodI01e8k6/VHdjrQKdwoAJKEAQBIKACShAEASCgAkoQBAEgoAJKEAQBIKACShAEASCgCkcmvK22+/3Vq8H+qFLOujo9bui/Pz8uyDjz5q7T49rXc8LZe90pnrzaY8e+fO7dbu/Vjvs4mIWK3q17z7One7+uvc944dN07uNM7RKeCKeOutN1vz+0ZfztOLZ63d3//Ru+XZh48etHbvrp6WZ8d9r11nHOs9P1Ozr2tqdgh1zhJxuH6iVglTd/4A5UfuFABIQgGAJBQASEIBgCQUAEhCAYAkFABIQgGAJBQASEIBgDRMxWfNX7t72lq8WC7rs4teNs1n9We75/N5b/eice7lqrV7vV6XZ4duXDfnF4t6dcV83lzeqBcYx14FwHp9Up7d7+s1FBERj5/U6x8iImaL+vs/rG61dm/G4/Ls00cftnZffNqofhkO0KOQmtUSzbqIQ0735xubG6unqff+3H/08z/j7hQASEIBgCQUAEhCAYAkFABIQgGAJBQASEIBgCQUAEhCAYAkFABI5QKcaaj39kREbFu1M73+jmdXl+XZfaOHJyJit6vPX2/3rd3TVN+9avYqDc2Op2Wr+6i3e2yUt0zNDplOf9T8+Ki1+7rZIxPz+vu5vt37/pyc3KwfY/ZJa/e4r1/zWf1jEhG93p52f9DQ7Ur6fBoafVND9L6bFe4UAEhCAYAkFABIQgGAJBQASEIBgCQUAEhCAYAkFABIQgGAVH6IfX5yq7e4UaOwOjpu7T6ORhXF1Xlr93azKc+ue10eMcWyPHt0805r9zDv5ftsXj9LNCs3OjUX4+66tfuo8TmczXvnjm39vY+IWCzr1RXruy/3zhL1CpVds52j1+bRXN6qrugevPs3bGd/s3Ljudn92XOnAEASCgAkoQBAEgoAJKEAQBIKACShAEASCgAkoQBAEgoAJKEAQCoXFC1PXmgtXjb6ctZHJ63ds6neC7M5v2zt3lzXe0pmzS6Wo/XN8uzpnVdbu8eh3jUVEbFr1LEMq3rHT0TEvHGU3dWz1u5Fp4Or0cEUETHFk9b8OByVZ+eL3jUcx3on1Dj0PodTo1tnal7Dnm7HzyE7gT6vuz977hQASEIBgCQUAEhCAYAkFABIQgGAJBQASEIBgCQUAEhCAYBULiRY37jXW7ya14e3Z63d77/7o/Ls06ePW7v3+115dmg+vb58dl6eHRe92op7r3ylNT+bN6pFjo9bu48bFSeboT4bETE26iLGqf5eRkTMole5MTVqTmbNGpL90KhbqX+NIyJi0fngNj/jU9QraIb28s/r37DdqpDDvT8Vn9erDMABCAUAklAAIAkFAJJQACAJBQCSUAAgCQUAklAAIAkFAJJQACCVS1N++9++3lo87Tbl2b/57l+2du82F+XZ1WLZ2r1vxGS3+6gzf/Xk49bu69NGl1FE3Pnl3yjPTsenrd2LWf2FzndHrd2bRv/NLnrv/dDpg4qI03W9t+mXXrjZ2n29uy7PTp/cau2ezuvz47ht7d6PV42DtFZHjN2in/p/0D3KNHX+RfeHonGO3uYSdwoAJKEAQBIKACShAEASCgAkoQBAEgoAJKEAQBIKACShAEAq11z84X///dbiq08fl2efPfygtfvps2f1c1zWZyMiYqzXcwzDvLV6vliXZ280qyX+/Te+2pv/3f9Qnn267Z1l1rgu28vz1u4nl/X6h32zh+T8We8sr778Qnn2G1/7Wmv39fVlefZ//o/e33bf+ev669xe178PERH7ff39Gcf6bERETPvW+GxWvy7TNLZ273a78uy47517PGA9R4U7BQCSUAAgCQUAklAAIAkFAJJQACAJBQCSUAAgCQUAklAAIAkFAFK5+2i+PG4tfunll8uzv/9f/nNr9/llvXfknQ/vt3ZvtvWul1mzeeTWjVvl2de/2usy+qP/9l9b81/6en3/ddTPHRFxclzveNpve91UH396UZ69HnvdR5fNHqb5ov7+f+lLv9rafXFRP8vHD77e2v3kSb2X7PLyqrV7vij/pMS47/UqRbMrabValWe7PWbbbf03aNeYjYgYx3oP09Ds96pwpwBAEgoAJKEAQBIKACShAEASCgAkoQBAEgoAJKEAQBIKACShAEAqF5UsT45ai5fLet689mu9np8/+aOT8uyDR/Wel4iID588Lc+enddnIyK+/Eq9D+obr32ptfuXXvpCa36/vFGeHWLZ2j07qs9v9r2/S6ZZvevl3osvtnbvx17H08OHD8qzm02vQ2jX6L/Z7HrdOmeNXqWzs95nfNxty7PbzWVrd4y917lsdR/1OoT2+319eOp1pEXjLKtl77tZ4U4BgCQUAEhCAYAkFABIQgGAJBQASEIBgCQUAEhCAYAkFABI5ZqLo8Yj4xER0XhqfLW+2Vr96mtfLs9+5fVvtXavGk+N/+TtH7d237x1uzx797RXKxKL3uPuq3V9/3WzimLZuIibTe91nt6oVwbcvlWv8oiI2HWqCyLi008+Kc9OU722IiJi2agKObvctHa/d//j8uz5k/prjIi4blRoTLuL1u5pqldodE3NKoqx8X5OjcqSrvmi/BNe5k4BgCQUAEhCAYAkFABIQgGAJBQASEIBgCQUAEhCAYAkFABIQgGAVC7OWDW7j5azevnR1dDrbrlsVKBsr3p9Nidj/dzLxbq1O4Z6z8/R0Wlr9bLRZRQRMc4bnSnz3ns/zObl2W7nzKLR8bTZ9N777a433/j6xPHxSWvzLupnmc16f9vtWq+zUWIWEfN5/b0fx965p6m+OyJibHRZNT+GMXSuy9B9nfXD7Nuf2Z/PnQIASSgAkIQCAEkoAJCEAgBJKACQhAIASSgAkIQCAEkoAJDKz+lPjUf6IyLmjRqF+apX0XDaGL/Y9R7TH/ZjefaFey+2di9ObpVnl+tetUS36mCzrc/vht41nIZdfXezXmBsnGU71c8RETEsmp+VxnzzZUbn77XlsvlZGepVITH1PlfDUH+l3WqJqVFB87P99VqMIerf+4iIqVFDckhD87tZ4U4BgCQUAEhCAYAkFABIQgGAJBQASEIBgCQUAEhCAYAkFABIQgGAVC4omg29fqJdo9RmcXTa2r0+rs83629ivayfe7frLd8vjuu75728ng/1npeIiOvG0bfdkppZvUfmarNprV4d1T+Hs+Y17PbILBb1fq+p2X7U6bK6fftOa/e8ce792OsE+uybeH5xvbez2Xs11N+fqfn96cx3d1e4UwAgCQUAklAAIAkFAJJQACAJBQCSUAAgCQUAklAAIAkFAFL5effLZh3BjeNVeXZY9LJpcXKzPHsa+9bu2aw+v33auybHxyfl2eVxs+ZiVq8uiIhYzOo9F1fPLlu7Z7Gsn2Ne/5xERMRYvy5D9Ko/tttebcnV1XV5dnej9zk8Xtev4X7frFsZ6/PT0KxoaMxPQ69CI4beNWy2ljQdbrmaCwCeG0IBgCQUAEhCAYAkFABIQgGAJBQASEIBgCQUAEhCAYAkFABIw3SI8gwAPpfcKQCQhAIASSgAkIQCAEkoAJCEAgBJKACQhAIASSgAkP4ff++1HYphWXYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_torch_image(X[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 4, kernel_size=3, stride=2, padding=1),    # 16x16x4\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 8, kernel_size=3, stride=2, padding=1),    # 8x8x8\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1),    # 4x4x16\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),    # 2x2x16\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16, 16, kernel_size=3, stride=2, padding=1),    # 4x4x16\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=3, stride=2, padding=1),    # 8x8x8\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 4, kernel_size=3, stride=2, padding=1),    # 16x16x4\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(4, 3, kernel_size=3, stride=2, padding=1),    # 32x32x3\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loss_func, valid_dl):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tot_loss,tot_acc,count = 0.,0.,0\n",
    "        for xb,yb in valid_dl:\n",
    "            pred = model(xb)\n",
    "            n = len(xb)\n",
    "            count += n\n",
    "            tot_loss += loss_func(pred,yb).item()*n\n",
    "    return tot_loss/count, tot_acc/count\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        model.train()\n",
    "        for xb,yb in train_dl:\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        validate(model, loss_func, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CIFARCustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
